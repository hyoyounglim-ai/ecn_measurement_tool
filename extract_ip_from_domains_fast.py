#!/usr/bin/env python3
import socket
import csv
import sys
import time
import logging
from datetime import date
import os
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import threading

# ë¡œê¹… ì„¤ì •
logging.basicConfig(stream=sys.stdout, level=logging.INFO)

# ìŠ¤ë ˆë“œ ì•ˆì „ì„ ìœ„í•œ ë½
results_lock = Lock()
progress_lock = Lock()

def extract_ip_from_domain(domain):
    """
    ë„ë©”ì¸ì—ì„œ IP ì£¼ì†Œë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    try:
        ip_addr = socket.gethostbyname(domain)
        return ip_addr
    except socket.gaierror:
        return None
    except Exception as e:
        return None

def process_domain_batch(domain_batch, batch_id):
    """
    ë„ë©”ì¸ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    batch_results = []
    for number, domain in domain_batch:
        ip_addr = extract_ip_from_domain(domain)
        if ip_addr:
            batch_results.append([number, domain, ip_addr])
        else:
            batch_results.append([number, domain, "N/A"])
    return batch_results

def count_total_lines(file_path):
    """
    íŒŒì¼ì˜ ì´ ë¼ì¸ ìˆ˜ë¥¼ ê³„ì‚°
    """
    try:
        with open(file_path, 'r') as f:
            return sum(1 for line in f if line.strip())
    except Exception as e:
        logging.error(f"íŒŒì¼ ë¼ì¸ ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
        return 0

def load_progress(progress_file):
    """
    ì§„í–‰ìƒí™© íŒŒì¼ì—ì„œ ì´ì „ ì§„í–‰ìƒí™© ë¡œë“œ
    """
    try:
        if os.path.exists(progress_file):
            with open(progress_file, 'r') as f:
                progress_data = json.load(f)
                logging.info(f"ì´ì „ ì§„í–‰ìƒí™© ë°œê²¬: {progress_data['processed_count']}ê°œ ì²˜ë¦¬ë¨")
                return progress_data
    except Exception as e:
        logging.error(f"ì§„í–‰ìƒí™© íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
    return None

def save_progress(progress_file, processed_count, success_count):
    """
    ì§„í–‰ìƒí™©ì„ íŒŒì¼ì— ì €ì¥
    """
    try:
        with progress_lock:
            progress_data = {
                'processed_count': processed_count,
                'success_count': success_count,
                'timestamp': time.time()
            }
            with open(progress_file, 'w') as f:
                json.dump(progress_data, f)
    except Exception as e:
        logging.error(f"ì§„í–‰ìƒí™© ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

def save_results_to_csv(output_file, results, is_temp=False):
    """
    ê²°ê³¼ë¥¼ CSV íŒŒì¼ì— ì €ì¥
    """
    try:
        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Number', 'Domain', 'IP_Address'])
            writer.writerows(results)
        
        if is_temp:
            logging.info(f"ì„ì‹œ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
        else:
            logging.info(f"ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
        return True
    except Exception as e:
        logging.error(f"CSV íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def process_domain_file_fast(input_file, output_file, max_workers=50):
    """
    ë©€í‹°ìŠ¤ë ˆë”©ì„ ì‚¬ìš©í•œ ë¹ ë¥¸ ë„ë©”ì¸ ì²˜ë¦¬
    """
    # íŒŒì¼ëª… ìƒì„±
    base_name = os.path.splitext(os.path.basename(input_file))[0]
    progress_file = f"progress_{base_name}.json"
    temp_file = f"temp_{base_name}.csv"
    
    # ì´ì „ ì§„í–‰ìƒí™© í™•ì¸
    progress_data = load_progress(progress_file)
    start_line = 0
    processed_count = 0
    success_count = 0
    results = []
    
    if progress_data:
        start_line = progress_data['processed_count']
        processed_count = progress_data['processed_count']
        success_count = progress_data['success_count']
        
        # ì„ì‹œ íŒŒì¼ì—ì„œ ì´ì „ ê²°ê³¼ ë¡œë“œ
        if os.path.exists(temp_file):
            try:
                with open(temp_file, 'r', encoding='utf-8') as csvfile:
                    reader = csv.DictReader(csvfile)
                    for row in reader:
                        results.append([row['Number'], row['Domain'], row['IP_Address']])
                logging.info(f"ì„ì‹œ íŒŒì¼ì—ì„œ {len(results)}ê°œ ê²°ê³¼ ë¡œë“œë¨")
            except Exception as e:
                logging.error(f"ì„ì‹œ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
                results = []
    
    # ì´ ë¼ì¸ ìˆ˜ ê³„ì‚°
    total_lines = count_total_lines(input_file)
    if total_lines == 0:
        logging.error("íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    logging.info(f"ë„ë©”ì¸ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {input_file}")
    logging.info(f"ì´ ì²˜ë¦¬í•  ë„ë©”ì¸ ìˆ˜: {total_lines:,}ê°œ")
    logging.info(f"ìŠ¤ë ˆë“œ ìˆ˜: {max_workers}ê°œ")
    if start_line > 0:
        logging.info(f"ì´ì „ ì§„í–‰ìƒí™©ë¶€í„° ì¬ì‹œì‘: {start_line:,}ë²ˆì§¸ ë¼ì¸ë¶€í„°")
    
    print(f"\n{'='*60}")
    print(f"ì§„í–‰ìƒí™©: {(processed_count/total_lines*100):.1f}% | ì²˜ë¦¬ëœ ë„ë©”ì¸: {processed_count:,}/{total_lines:,} | ì„±ê³µ: {success_count:,}ê°œ")
    
    # ë„ë©”ì¸ ë°°ì¹˜ ì¤€ë¹„
    domain_batches = []
    current_batch = []
    batch_size = 100  # ë°°ì¹˜ í¬ê¸°
    
    try:
        with open(input_file, "r") as f:
            for line_num, line in enumerate(f, 1):
                if line_num <= start_line:
                    continue
                
                line = line.strip()
                if not line:
                    continue
                
                try:
                    parts = line.split(',')
                    if len(parts) >= 2:
                        number = parts[0].strip()
                        domain = parts[1].strip()
                        current_batch.append((number, domain))
                        
                        if len(current_batch) >= batch_size:
                            domain_batches.append(current_batch)
                            current_batch = []
                except Exception as e:
                    logging.error(f"ë¼ì¸ {line_num} íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ë§ˆì§€ë§‰ ë°°ì¹˜ ì¶”ê°€
        if current_batch:
            domain_batches.append(current_batch)
    
    except Exception as e:
        logging.error(f"íŒŒì¼ ì½ê¸° ì¤‘ ì˜¤ë¥˜: {e}")
        return False
    
    logging.info(f"ì´ {len(domain_batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í• ë¨")
    
    # ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        
        for i, batch in enumerate(domain_batches):
            future = executor.submit(process_domain_batch, batch, i)
            futures.append(future)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        for i, future in enumerate(as_completed(futures)):
            try:
                batch_results = future.result()
                
                with results_lock:
                    results.extend(batch_results)
                    processed_count += len(batch_results)
                    success_count += sum(1 for r in batch_results if r[2] != "N/A")
                
                # ì§„í–‰ìƒí™© í‘œì‹œ
                if processed_count % 1000 == 0:
                    progress_percent = (processed_count / total_lines) * 100
                    failed_count = processed_count - success_count
                    print(f"\rì§„í–‰ìƒí™©: {progress_percent:.1f}% | ì²˜ë¦¬ëœ ë„ë©”ì¸: {processed_count:,}/{total_lines:,} | ì„±ê³µ: {success_count:,}ê°œ | ì‹¤íŒ¨: {failed_count:,}ê°œ", end='', flush=True)
                    
                    # ì„ì‹œ ì €ì¥
                    save_results_to_csv(temp_file, results, is_temp=True)
                    save_progress(progress_file, processed_count, success_count)
                
            except Exception as e:
                logging.error(f"ë°°ì¹˜ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    print(f"\n{'='*60}")
    logging.info("IP ì¶”ì¶œ ì™„ë£Œ. ìµœì¢… ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    if save_results_to_csv(output_file, results):
        # ì„ì‹œ íŒŒì¼ê³¼ ì§„í–‰ìƒí™© íŒŒì¼ ì‚­ì œ
        try:
            if os.path.exists(temp_file):
                os.remove(temp_file)
            if os.path.exists(progress_file):
                os.remove(progress_file)
            logging.info("ì„ì‹œ íŒŒì¼ ë° ì§„í–‰ìƒí™© íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
        except Exception as e:
            logging.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        print(f"\n{'='*60}")
        logging.info(f"ğŸ“Š ìµœì¢… í†µê³„:")
        logging.info(f"   ì´ ì²˜ë¦¬ëœ ë„ë©”ì¸: {processed_count:,}ê°œ")
        logging.info(f"   ì„±ê³µì ìœ¼ë¡œ IP ì¶”ì¶œëœ ë„ë©”ì¸: {success_count:,}ê°œ")
        logging.info(f"   ì‹¤íŒ¨í•œ ë„ë©”ì¸: {processed_count - success_count:,}ê°œ")
        logging.info(f"   ì„±ê³µë¥ : {(success_count/processed_count*100):.1f}%")
        
        return True
    else:
        return False

def main():
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python extract_ip_from_domains_fast.py <ë„ë©”ì¸_ë¦¬ìŠ¤íŠ¸_íŒŒì¼> [ìŠ¤ë ˆë“œ_ìˆ˜]")
        print("ì˜ˆì‹œ: python extract_ip_from_domains_fast.py websitelist/web_1000.txt 50")
        sys.exit(1)
    
    input_file = sys.argv[1]
    max_workers = int(sys.argv[2]) if len(sys.argv) > 2 else 50
    
    # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
    base_name = os.path.splitext(os.path.basename(input_file))[0]
    today = date.today().strftime("%Y%m%d")
    output_file = f"ip_extracted_{base_name}_{today}.csv"
    
    start_time = time.time()
    
    success = process_domain_file_fast(input_file, output_file, max_workers)
    
    if success:
        print(f"\n{'='*60}")
        print(f"âœ… IP ì¶”ì¶œ ì™„ë£Œ!")
        print(f" ì…ë ¥ íŒŒì¼: {input_file}")
        print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_file}")
        print(f"â±ï¸  ì²˜ë¦¬ ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
        print(f"{'='*60}")
    else:
        print("âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)

if __name__ == "__main__":
    main() 